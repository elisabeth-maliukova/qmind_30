{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import praw\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Opening JSON file\n",
    "f = open('client_secrets_liz.json')\n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "data = json.load(f)\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=data['client_id'],\n",
    "    client_secret=data['client_secret'],\n",
    "    user_agent=data['user_agent']\n",
    ")\n",
    "\n",
    "url_link = input(\"Enter your Reddit thread URL: \")\n",
    "#url = \"https://www.reddit.com/r/esist/comments/6g18xv/theres_so_much_more_about_trump_to_investigate/\"\n",
    "submission = reddit.submission(url=url_link)\n",
    "title = submission.title\n",
    "rawComments = []\n",
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    rawComments.append(comment.body)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rawComments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
    "analyzedData = []\n",
    "for index in df.index:\n",
    "    comment = df[0][index]\n",
    "    analysis = classifier(comment, truncation=True)\n",
    "    analyzedData.append([comment, analysis])\n",
    "\n",
    "sentiment_comments = pd.DataFrame(analyzedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_allowed_top_label(sentiment_analysis, allowed_labels):\n",
    "    if isinstance(sentiment_analysis, list) and len(sentiment_analysis) > 0 and isinstance(sentiment_analysis[0][0], dict):\n",
    "        top_label = sentiment_analysis[0][0].get('label')\n",
    "        return any(top_label == allowed_label for allowed_label in allowed_labels)\n",
    "    return False\n",
    "\n",
    "def has_allowed_keywords(comment_text, keywords):\n",
    "    return any(keyword in comment_text.lower() for keyword in keywords)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ZachBeesley/toxic-comments\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ZachBeesley/toxic-comments\", from_tf=True)\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, max_length=512)\n",
    "\n",
    "def is_toxic(comment_text,classifier=classifier):\n",
    "    result = classifier(comment_text)[0]\n",
    "    return result['label'] == 'Toxic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Elisabeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Elisabeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Elisabeth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_with_lemmatization(text):\n",
    "    # convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # split text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # remove stopwords - remove words like a, and, at, etc\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # lemmatization - break down a word to its root meaning\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    text = ' '.join(lemmatized_tokens)\n",
    "    return text\n",
    "\n",
    "allowed_keywords = preprocess_with_lemmatization(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['disapproval', 'disappointment', 'anger', 'disgust']\n",
    "filtered_data = []\n",
    "for index, row in sentiment_comments.iterrows():\n",
    "    sentiment_analysis = row[1] if pd.notna(row[1]).any else []\n",
    "    result = has_allowed_top_label(sentiment_analysis, allowed_labels=labels)\n",
    "    filtered_data.append(result)\n",
    "\n",
    "filtered_data_keywords = sentiment_comments[0].apply(lambda x: has_allowed_keywords(x.lower() if pd.notna(x) else \"\", allowed_keywords))\n",
    "# remove the comments without relevant keywords from title as well as comments with labels not under the labels listed above \n",
    "filtered_df = sentiment_comments[filtered_data & filtered_data_keywords & ~(sentiment_comments[0].astype(str).isin(['[removed]', '[deleted]']))]\n",
    "#check for toxic comments - done on smaller subset of comments as to improve efficiency\n",
    "toxicity_data = filtered_df[0].apply(lambda x: is_toxic(x) if pd.notna(x) else False)\n",
    "filtered_df = filtered_df[~toxicity_data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fckin Akon says he doesn't agree with Kanye's comments on Hitler . he says if you overlook the fact that he didn't seem to take issue with antisemitic conspiracy theories or the Holocaust denial, Akon is being pretty reasonable here . He says anyone using the excuse is racist because they ignore the how complex racism is .\n"
     ]
    }
   ],
   "source": [
    "reasons_list = filtered_df[0].tolist()\n",
    "full_corpus = \"\\n\".join(reasons_list)\n",
    "summarizer = pipeline(\"summarization\", model=\"Falconsai/text_summarization\")\n",
    "summarized = summarizer(full_corpus, max_length=1000, min_length=50, do_sample=False)\n",
    "print(summarized[0].get('summary_text'))\n",
    "\n",
    "#some urls to use:\n",
    "#https://www.reddit.com/r/entertainment/comments/zd5n4x/kanye_west_antisemitism_rapper_akon_backs_west/\n",
    "#akon backed kanye west on new album drop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
